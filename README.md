<div align="center">

# ğŸ¤– Affective AI â€” Understanding Emotions through Your Voice

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-Enabled-red)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![OpenAI](https://img.shields.io/badge/OpenAI-Whisper-blue.svg)](https://openai.com/research/whisper)
[![XGBoost](https://img.shields.io/badge/XGBoost-Integrated-orange.svg)](https://xgboost.ai/)

*Empowering machines to understand human emotions through Audio data.*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Tech Stack](#-tech-stack) â€¢ [License](#-license) â€¢ [Contact](#-contact)

</div>

---

## ğŸŒŸ Overview

**Affective AI** is a cutting-edge application designed to detect and analyze emotions from textual inputs. Leveraging advanced natural language processing and machine learning techniques, it provides insights into the emotional undertones present in text, aiding in areas like sentiment analysis, customer feedback interpretation, and more.

---

## ğŸ“Œ Features

- âœ… **Emotion Detection**: Analyze text to identify underlying emotions.
- âœ… **Real-time Analysis**: Immediate feedback on emotional content.
- âœ… **User-friendly Interface**: Intuitive design for seamless interaction.
- âœ… **Data Visualization**: Graphical representation of emotion metrics.
- âœ… **Session Tracking**: Monitor and log user interactions for insights.

---

## ğŸ› ï¸ Installation

#### Repository Cloning

```bash
# Clone the repository
git clone https://github.com/priyam-hub/Affective-AI.git

# Navigate into the directory
cd Affective-AI
```

#### Environmental Setup and Dependency Installation

```bash
# Environmental Setup and Dependency Installation
bash setup.sh
```
---

## ğŸš€ Usage

```bash
# Run the Streamlit application
streamlit run app.py
```

Upon running, navigate to the provided local URL in your browser to interact with the Affective AI application.

---

## ğŸ§  Tech Stack

### Core Technologies
- **Python 3.10+** -  Primary programming language.
- **OpenAI Whisper** -  Advanced speech recognition model.
- **Pandas & NumPy** -  Data manipulation and numerical operations.
- **XGBoost** -  Optimized gradient boosting library for classification tasks.
- **Plotly & Altair** -  Interactive data visualization libraries.
- **Streamlit** - Framework for building interactive web applications.

### Models Stack
- **Audio Generation**: OpenAI Whisper-Base
- **Emotion Generation**: Multinomial Logistic Regression, Multinomial Naive Bayes, Random Forest

### ğŸ”Š Whisper Base Model (Fine-tuned)

This project uses a fine-tuned version of OpenAI's Whisper base model for robust and efficient speech-to-text transcription. It has been trained to handle conversational audio with improved accuracy.

ğŸ‘‰ [View on Hugging Face](https://huggingface.co/priyammmmm/whisper_base)

---


## ğŸ“‹ Prerequisites

Before you begin using the AI-Powered Emotion Detection System, ensure that your environment is properly set up. You will need to install the following tools and libraries:

### System and Software Requirements:
- **Operating System**: Linux, macOS, or Windows (All platforms supported)
- **Python** (3.10+): This project is built with Python 3.10 or newer. You can download Python from the official website:
   - [Python Download](https://www.python.org/downloads/)

---

## ğŸ—ºï¸ Roadmap

<details>
  <summary><strong>ğŸ“ Phase 1: Foundational Setup <em>(0 - 2 months)</em></strong></summary>

* ğŸ™ï¸ **Audio Input & STT Integration**

  * Integrate OpenAI Whisper for speech-to-text conversion.
* ğŸ§¹ **Text Preprocessing**

  * Clean and structure text data from transcribed audio.
* ğŸ’¬ **Basic Emotion Detection**

  * Implement baseline models:

    * Multinomial Naive Bayes
    * Logistic Regression
* ğŸ–¥ï¸ **Streamlit Integration**

  * Build initial user interface for interaction.

</details>

<details>
  <summary><strong>ğŸš€ Phase 2: Model Enhancement & Visualization <em>(2 - 4 months)</em></strong></summary>

* ğŸ¤– **Advanced Emotion Detection Models**

  * Integrate:

    * Random Forest (with XGBoost)
    * Multilayer Perceptron
* ğŸ“Š **Real-Time Feedback & Visualization**

  * Use Plotly/Altair for interactive emotion graphs.
* ğŸ—‚ï¸ **User Session Tracking**

  * Log user inputs and detected emotions in a local SQLite database.

</details>

<details>
  <summary><strong>ğŸŒ Phase 3: Expansion & Deployment <em>(4 - 6 months)</em></strong></summary>

* ğŸŒ **Multi-language Emotion Detection**

  * Expand to multilingual transcription and analysis.
* ğŸ§  **Personalized Feedback Engine**

  * Analyze historical data for tailored emotion insights.
* ğŸ“¦ **Containerization & Cloud Deployment**

  * Dockerize app
  * Deploy using AWS/GCP/Azure

</details>

---

## ğŸ“ Project Structure

```plaintext
Affective-AI/
â”œâ”€â”€ .gitignore                                # Ignoring files for Git
â”œâ”€â”€ app.py                                    # Streamlit App
â”œâ”€â”€ Dockerfile                                # Stored the Docker Setup
â”œâ”€â”€ LICENSE                                   # MIT License
â”œâ”€â”€ main.py                                   # Run the Machine Learning Pipeline
â”œâ”€â”€ README.md                                 # Project documentation
â”œâ”€â”€ requirements.txt                          # Python dependencies
â”œâ”€â”€ setup.sh                                  # Package installation configuration
â”œâ”€â”€ test.py                                   # Run the Machine Learning Pipeline
â”œâ”€â”€ .devcontainer/                            # Directory to store the containers for Docker
â”‚   â””â”€â”€ devcontainer.json                     # Docker Container configuration
â”œâ”€â”€ .streamlit/                               # Directory to store configuration of Streamlit
â”‚   â””â”€â”€ config.toml                           # Streamlit Configuration
â”œâ”€â”€ .vscode/                                  # Directory to store configuration of VS Code
â”‚   â””â”€â”€ settings.json                         # VS Code configuration
â”œâ”€â”€ audio/                                    # Directory to store the recorded audio file
â”‚   â””â”€â”€ recorded_audio.wav                    # Recorded audio stored in .wav format
â”œâ”€â”€ config/                                   # Configuration files
â”‚   â”œâ”€â”€ __init__.py                           
â”‚   â””â”€â”€ config.py/                            # All Configuration Variables of Pipeline
â”œâ”€â”€ docs/                                     # Documents Directory
â”œâ”€â”€ data/                                     # Data directory
â”‚   â”œâ”€â”€ emotion_dataset_cleaned.csv           # Cleaned Emotion Dataset
â”‚   â””â”€â”€ emotion_dataset_raw.csv               # Raw Emotion Dataset
â”œâ”€â”€ database/                                 # Directory to store the behaviour of the Client
â”‚   â””â”€â”€ data.db                               # Database file to store the behaviour of the user
â”œâ”€â”€ images/                                   # Image Directory
â”‚   â”œâ”€â”€ home_banner.jpg                       # Image for Home page
â”‚   â””â”€â”€ sidebar_logo.jpg                      # Logo for Sidebar
â”œâ”€â”€ models/                                   # Directory to store the Models
â”‚   â”œâ”€â”€ grammarly/                            # Directory to store the Grammarly Model
|   â”œâ”€â”€ whisper/                              # Directory to store the Base Model of OpenAI-Whisper
â”‚   â””â”€â”€ classification_models/                # Directory to store the ML Classification Models
â”‚       â”œâ”€â”€ linear_svc.pkl                         # Pickle file for Linear Support Vector Classifier
â”‚       â”œâ”€â”€ mlp_classifier.pkl                     # Pickle file for Multilayer Perceptron
â”‚       â”œâ”€â”€ multinomial_logistic_regression.pkl    # Pickle file for Multinomial Logistic Regression
â”‚       â”œâ”€â”€ multinomial_naive_bayes.pkl            # Pickle file for Multinomial Naive Bayes
â”‚       â”œâ”€â”€ random_forest.pkl                      # Pickle file for Random Forest with XGBoost
â”‚       â””â”€â”€ rbf_svm.pkl                            # Pickle file for Support Vector Machine with RBF Kernel
â”œâ”€â”€ notebooks/                                # Jupyter notebooks for experimentation
â”‚   â””â”€â”€ emotion_detection.ipynb               # Experimented Emotion Detection in Jupyter Notebook
â”œâ”€â”€ results/                                  # Reports of the Project
â”‚   â”œâ”€â”€ eda_results/                          # Directory to store the results of EDA
â”‚   â””â”€â”€ model_accuracy_comparison.png         # Result of different ML Models Accuracy
â”œâ”€â”€ src/                                      # Source code
â”‚   â”œâ”€â”€ audio_recorder/                       # Audio Recorder Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”‚   â””â”€â”€ record_audio.py                   # Python file to record Audio                                           
â”‚   â”œâ”€â”€ audio_transcriber/                    # Audio Transcriber Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py   
â”‚   â”‚   â””â”€â”€ transcribe_audio.py               # Python file to transcribe Audio                                      
â”‚   â”œâ”€â”€ data_cleaner/                         # Data Cleaner Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py                           
â”‚   â”‚   â””â”€â”€ cleaner.py                        # Python File to clean the data                  
â”‚   â”œâ”€â”€ exploratory_data_analysis/            # Exploratory Data Analysis Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py  
â”‚   â”‚   â””â”€â”€ exploratory_data_analyzer.py      # Python file to perform EDA                                              
â”‚   â”œâ”€â”€ pipeline/                             # ML Pipeline Directory
â”‚   â”‚   â”œâ”€â”€ __init__.py   
â”‚   â”‚   â””â”€â”€ model_pipeline.py                 # Python file to Create the pipeline                                               
â”‚   â””â”€â”€ utils/                                # Utility Functions Directory
â”‚       â”œâ”€â”€ __init__.py                     
â”‚       â”œâ”€â”€ audio_saver.py                    # Audio save and loading feature
â”‚       â”œâ”€â”€ data_loader.py                    # Data save and loading feature
â”‚       â”œâ”€â”€ logger.py                         # Logger Setup
â”‚       â”œâ”€â”€ model_loader.py                   # Model save and loading feature
â”‚       â”œâ”€â”€ pipeline_saver.py                 # Save the Pipeline
â”‚       â””â”€â”€ save_plot.py                      # Save the plot into particular directory       
â””â”€â”€ web/
    â”œâ”€â”€ __init__.py  
    â”œâ”€â”€ pages/                                # Directory for the Different Streamlit Pages
    â”‚   â”œâ”€â”€ __init__.py 
    â”‚   â”œâ”€â”€ about.py                          # About Page
    â”‚   â”œâ”€â”€ home.py                           # Home Page
    â”‚   â”œâ”€â”€ monitor.py                        # Monitor Page
    â”‚   â””â”€â”€ speech_to_text.py                 # STT Page
    â””â”€â”€ utils/                                # Directory for the utility functions of Streamlit Pages
        â”œâ”€â”€ __init__.py   
        â””â”€â”€ database_manager.py               # Python file utility functions of the Web App
```
---

## ğŸ”® Future Work

To enhance the capabilities of **Affective AI** and scale it into a more robust emotion-aware system, we have outlined the development roadmap in the following phases:

### ğŸš€ **Phase 1: Immediate Enhancements** *(Timeframe: 1â€“2 months)*

* âœ… **Multi-language Support**: Integrate multilingual transcription using Whisper's multilingual capabilities.
* âœ… **User Authentication**: Add login/signup functionality for personalized tracking.
* âœ… **Emotion Confidence Scores**: Display confidence/probability of detected emotions.
* âœ… **Custom Dataset Upload**: Allow users to upload their own datasets for model training.
* âœ… **UI/UX Improvements**: Make UI more dynamic and responsive across devices.


### ğŸ”§ **Phase 2: Intermediate Upgrades** *(Timeframe: 2â€“4 months)*

* âœ… **Contextual Emotion Tracking**: Track emotion changes across longer sessions or conversations.
* âœ… **Real-Time Audio Stream Analysis**: Detect emotion from live audio streams instead of recorded files.
* âœ… **Mobile Responsiveness**: Optimize the platform for mobile browsers and potential app integration.
* âœ… **Cloud Database Integration**: Move from local `data.db` to cloud-based storage (e.g., Firebase, MongoDB Atlas).
* âœ… **Model Fine-Tuning**: Train emotion classification models on larger and more diverse datasets for improved accuracy.


### ğŸŒ **Phase 3: Advanced Capabilities and Deployment** *(Timeframe: 4â€“6 months)*

* âœ… **Cross-Modal Emotion Detection**: Combine audio and facial expression analysis (audio + video input).
* âœ… **Dashboard Analytics**: Provide detailed user-based emotional analytics and trends over time.
* âœ… **Deployment as SaaS**: Launch Affective AI as a Software-as-a-Service (SaaS) platform for enterprises.
* âœ… **API Development**: Build RESTful APIs for third-party emotion detection integrations.
* âœ… **Voice Emotion Feedback Loop**: Allow users to manually verify/correct emotions to improve future model accuracy.

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for more details.

---

<div align="center">

**Made by Priyam Pal**

[â†‘ Back to Top](#-affective-ai--understanding-emotions-through-text)

</div>
